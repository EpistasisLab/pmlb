---
title: "Penn Machine Learning Benchmarks"
---


Penn Machine Learning Benchmarks (PMLB) is a large collection of curated benchmark datasets for evaluating and comparing supervised machine learning algorithms.
These datasets cover a broad range of applications including binary/multi-class classification and regression problems as well as combinations of categorical, ordinal, and continuous features.


## Summary statistics

In the interactive  [plotly](https://plotly.com/) chart below, each dot represents a dataset colored based on its associated task (classification vs. regression).
In log scale, the *x* and *y* axis shows the number of observations and features respectively.
Please click on the legend to hide/show the groups of datasets.
Click on each dot to access the dataset's [pandas-profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/) report.

*Note*: If a dataset has more than 20 features, we randomly chose 20 to be displayed in its profiling report. Therefore, please disregard the `Number of variables` in the corresponding report and, instead, use the correct `n_features` in the chart and table below.

```{r setup, include=FALSE}
library(plotly)
library(dplyr)
library(htmltools)
library(htmlwidgets)
```

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=8.5}
sum_stats <- readr::read_tsv('../pmlb/all_summary_stats.tsv') %>% 
  mutate(urls = paste0(
    'https://epistasislab.github.io/pmlb/profile/',
    dataset,
    '.html'),
    Metadata = paste0('https://github.com/EpistasisLab/pmlb/tree/master/datasets/', dataset, '/metadata.yaml'))

ply_dat <- sum_stats %>% 
  add_count(task) %>% 
  mutate(Task = paste0(task, ' (', n, ')')) 

ggp <- ply_dat %>% 
  select(n_observations = n_instances, n_features, Task) %>% 
  ggplot(aes(x = n_observations, y = n_features, color = Task)) +
  geom_point(alpha = 0.6, size = 2) +
  theme_bw() +
  theme(legend.title = element_blank()) +
  scale_x_log10(name = 'Number of observations',
                ) +
  scale_y_log10(name = 'Number of features') +
  scale_color_manual(values = c('#ca446f', '#009392')) +
  NULL
ply <- plotly_build(ggp)


class_dats <- sum_stats %>% 
  filter(task == 'classification') %>% 
  pull(dataset)
fig_lay1 <- gsub(pattern = '<br \\/>Task: classification \\([0-9]+\\)', replacement = '', ply$x$data[[1]]$text) %>% 
  gsub(pattern = 'n_observations:\\s+', '', .) %>% 
  gsub(pattern = '<br \\/>n_features:\\s+', ' observations<br />', .) %>% 
  paste0(' features')

ply$x$data[[1]]$text <- paste0('<b>', class_dats, '</b><br /><br />', fig_lay1)

reg_dats <- sum_stats %>% 
  filter(task == 'regression') %>% 
  pull(dataset)
fig_lay2 <- gsub(pattern = '<br \\/>Task: regression \\([0-9]+\\)', replacement = '', ply$x$data[[2]]$text) %>% 
  gsub(pattern = 'n_observations:\\s+', '', .) %>% 
  gsub(pattern = '<br \\/>n_features:\\s+', ' observations<br />', .) %>% 
  paste0(' features')

ply$x$data[[2]]$text <- paste0('<b>', reg_dats, '</b><br /><br />', fig_lay2)


ply$elementId <- "PlotlyGraph"

javascript <- HTML(paste("var myPlot = document.getElementById('", ply$elementId, "');
                           myPlot.on('plotly_click', function(data){
                           var urls = ", jsonlite::toJSON(split(ply_dat, ply_dat$Task)), ";
                           window.open(urls[data.points[0].data.name][data.points[0].pointNumber]['urls'],'_blank');
                           });", sep=''))  

ply <- prependContent(ply, onStaticRenderComplete(javascript))
ply
```

Browse, sort, filter and search the complete table of summary statistics below.

* Click on the dataset's name to access its [pandas-profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/) report.

* Click on the GitHub Octocat <i class="fab fa-github"></i> to access its metadata.

* To filter, please type in the box at the bottom of each numeric column in the format `low ... high`.
For example, if you want to see all *classification datasets with 80 to 100 observations*, select `classification` at the bottom of `Task` and type `80 ... 100` at the bottom of the `n_observations` column. 


```{r echo=FALSE}
my_data <- sum_stats %>% 
  mutate(Metadata = paste0("<a href='", Metadata,"'>", 
                           "<i class=\"fab fa-github\"></i>","</a>"),
         dataset = paste0("<a href='", urls,"'>", dataset,"</a>"),
         # Dataset = paste(dataset, Metadata),
         Imbalance = round(imbalance, 2),
         n_classes = ifelse(endpoint_type == 'continuous', NA, `n_classes`),
         Task = as.factor(task),
         Endpoint = as.factor(endpoint_type)) %>% 
  arrange(task, dataset) %>% 
  select(Dataset = dataset, n_observations = n_instances, n_features = n_features, 
         n_classes, Endpoint, Imbalance, Task, Metadata)

DT::datatable(
  my_data,
   escape = FALSE,
  extensions = 'Buttons', options = list(
    dom = 'Blfrtip',
    buttons = c('csv'),
    columnDefs = list(list(className = 'dt-center', targets = 1:7))
  ),
  filter = list(position = 'bottom', clear = FALSE, plain = TRUE),
  rownames= FALSE,
)
```

The [complete table](https://github.com/EpistasisLab/pmlb/blob/master/pmlb/all_summary_stats.tsv) of dataset characteristics is also available for download.
Please note, in our documentation, a feature is considered:

* "binary" if it is of type integer and has 2 unique values (equivalent to pandas profiling's "boolean")
* "categorical" if it is of type integer and has *more than* 2 unique values (equivalent to pandas profiling's "categorical")
* "continuous" if it is of type float (equivalent to pandas profiling's "numeric").


## Dataset format

All datasets are stored in a common format:

* First row is the column names
* Each following row corresponds to one observation of the data
* The dependent variable/endpoint/outcome column is named `target`
* All columns are tab (`\t`) separated
* All files are compressed with `gzip` to conserve space


## Citing PMLB

If you use PMLB in a scientific publication, please consider citing one of the following papers:

Joseph D. Romano, Le, Trang T., William La Cava, John T. Gregg, Daniel J. Goldberg, Praneel Chakraborty, Natasha L. Ray, Daniel Himmelstein, Weixuan Fu, and Jason H. Moore.
[PMLB v1.0: an open source dataset collection for benchmarking machine learning methods](https://arxiv.org/abs/2012.00058).
_arXiv preprint arXiv:2012.00058_ (2020).

```bibtex
@article{romano2021pmlb,
  title={PMLB v1.0: an open source dataset collection for benchmarking machine learning methods},
  author={Romano, Joseph D and Le, Trang T and La Cava, William and Gregg, John T and Goldberg, Daniel J and Chakraborty, Praneel and Ray, Natasha L and Himmelstein, Daniel and Fu, Weixuan and Moore, Jason H},
  journal={arXiv preprint arXiv:2012.00058v2},
  year={2021}
}
```

Olson, Randal S., William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore.
[PMLB: a large benchmark suite for machine learning evaluation and comparison](https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4).
BioData mining 10, no. 1 (2017): 1-13.
*BioData Mining* **10**, page 36.

BibTeX entry:

```bibtex
@article{Olson2017PMLB,
    author="Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.",
    title="PMLB: a large benchmark suite for machine learning evaluation and comparison",
    journal="BioData Mining",
    year="2017",
    month="Dec",
    day="11",
    volume="10",
    number="36",
    pages="1--13",
    issn="1756-0381",
    doi="10.1186/s13040-017-0154-4",
    url="https://doi.org/10.1186/s13040-017-0154-4"
}
```

## Support for PMLB

PMLB was developed in the [Computational Genetics Lab](http://epistasis.org/) at the [University of Pennsylvania](https://www.upenn.edu/) with funding from the [NIH](http://www.nih.gov/) under grant AI117694, LM010098 and LM012601.
We are grateful for the support of the NIH and the University of Pennsylvania during the development of this project.

